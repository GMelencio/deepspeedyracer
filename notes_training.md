## Training notes
Graph: Proving how model is good.
Log analysis
Real world, stability over speed. 
Small number of actions worked  well in virtual  league, not in Physical world.
Training methodology, data driven analysis.
Look for episodes with 100% completed.
Entropy vs Reward. Reward is going UP, Entropy is going down.
Entropy as policy of data model. 
Max progress should not stagnate. 
Percentage of Completed laps. Stable model have close to average.

Optimize angles:
Exploit angles about steering  to learn pattern.

Tutor: 2h is the best. until 6h is more to get better.
if oversteer punish, speed reward, stay on track reward.
Focues on reward function and action space.

- Clone a Trained Model to Start a New Training Pass
If you clone a previously trained model as the starting point of a new round of training, you could improve training efficiency. To do this, modify the hyperparameters to make use of already learned knowledge.